{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\刘衍程\\nlp\\一步一步来\\中文新闻分类准备\\资源\\搜狗新闻语料\\news_tensite_xml.smarty\\news.sohu.com.txt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a92f8b6b79d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mlabels_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_count\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "'''\n",
    "本文档负责实际读取语料库文件\n",
    "训练LR模型\n",
    "过程中保存词典、语料和训练后的模型\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.linear_model.logistic import  *\n",
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from learnTextClsf.textProcess import *\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "def get_stop_words():\n",
    "    path = r\"C:\\Users\\刘衍程\\nlp\\stopwords.txt\"\n",
    "    file = open(path, 'rb').read().decode('utf-8').split('\\r\\n')\n",
    "    return set(file)\n",
    "\n",
    "\n",
    "def rm_stop_words(word_list):\n",
    "    word_list = list(word_list)\n",
    "    stop_words = get_stop_words()\n",
    "    # 这个很重要，注意每次pop之后总长度是变化的\n",
    "    for i in range(word_list.__len__())[::-1]:\n",
    "        # 去停用词\n",
    "        if word_list[i] in stop_words:\n",
    "            word_list.pop(i)\n",
    "        #  去数字\n",
    "        elif word_list[i].isdigit():\n",
    "            word_list.pop(i)\n",
    "    return word_list\n",
    "def rm_word_freq_so_little(dictionary, freq_thred):\n",
    "    small_freq_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq < freq_thred ]\n",
    "    dictionary.filter_tokens(small_freq_ids)\n",
    "    dictionary.compactify()\n",
    "def listdir(path, list_name):\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            listdir(file_path, list_name)\n",
    "        else:\n",
    "            list_name.append(file_path)\n",
    "if __name__ == '__main__':\n",
    "    freq_thred = 10        # 当一个单词在所有语料中出现次数小于这个阈值，那么该词语不应被计入词典中\n",
    "\n",
    "    # 字典\n",
    "    dictionary = corpora.Dictionary()\n",
    "    # 词袋\n",
    "    bow  = []\n",
    "\n",
    "    labels_count = []\n",
    "    list_name = []\n",
    "    listdir(r'C:\\Users\\刘衍程\\nlp\\一步一步来\\中文新闻分类准备\\资源\\搜狗新闻语料\\news_tensite_xml.smarty', list_name)\n",
    "    count = 0\n",
    "\n",
    "    for path in list_name[0:2]:\n",
    "        print(path)\n",
    "\n",
    "        file = open(path, 'rb').read().decode('utf-8').split('\\n')\n",
    "        class_count = 0\n",
    "        for text in file:\n",
    "\n",
    "            # 打标签\n",
    "            class_count = class_count + 1\n",
    "\n",
    "            content = text\n",
    "            # 分词\n",
    "            word_list = list(jieba.cut(content, cut_all=False))\n",
    "            # 去停用词\n",
    "            word_list = rm_stop_words(word_list)\n",
    "\n",
    "            dictionary.add_documents([word_list])\n",
    "\n",
    "            '''\n",
    "            转化成词袋\n",
    "            gensim包中的dic实际相当于一个map\n",
    "            doc2bow方法，对没有出现过的词语，在dic中增加该词语\n",
    "            如果dic中有该词语，则将该词语序号放到当前word_bow中并且统计该序号单词在该文本中出现了几次\n",
    "            '''\n",
    "            word_bow = dictionary.doc2bow(word_list)\n",
    "            bow.append(word_bow)\n",
    "\n",
    "        labels_count.append(class_count-1)\n",
    "        for i in (bow.__len__()):\n",
    "            print(bow[i])\n",
    "\n",
    "    # with open('dictionary.pkl', 'wb') as f1:\n",
    "    #     pickle.dump(dictionary, f1)\n",
    "\n",
    "    # 去除过少单词 ps:可能导致维数不同\n",
    "    rm_word_freq_so_little(dictionary,freq_thred)\n",
    "\n",
    "    # dictionary.save('dicsave.dict')\n",
    "    # corpora.MmCorpus.serialize('bowsave.mm', bow)\n",
    "\n",
    "    tfidf_model = models.TfidfModel(corpus=bow,dictionary=dictionary)\n",
    "\n",
    "    # with open('tfidf_model.pkl', 'wb') as f2:\n",
    "    #     pickle.dump(tfidf_model, f2)\n",
    "    '''训练tf-idf模型'''\n",
    "    corpus_tfidf = [tfidf_model[doc] for doc in bow]\n",
    "\n",
    "    '''将gensim格式稀疏矩阵转换成可以输入scikit-learn模型格式矩阵'''\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    line_count = 0\n",
    "    for line in corpus_tfidf:\n",
    "        for elem in line:\n",
    "            rows.append(line_count)\n",
    "            cols.append(elem[0])\n",
    "            data.append(elem[1])\n",
    "        line_count += 1\n",
    "\n",
    "    print(line_count)\n",
    "    tfidf_matrix = csr_matrix((data,(rows,cols))).toarray()\n",
    "\n",
    "    count = 0\n",
    "    for ele in tfidf_matrix:\n",
    "        # print(ele)\n",
    "        # print(count)\n",
    "        count = count + 1\n",
    "\n",
    "    # cut label 1 mil label 0\n",
    "    '''生成labels'''\n",
    "    labels = np.zeros(sum(labels_count) + 1)\n",
    "    for i in range(labels_count[0]):\n",
    "        labels[i] = 1\n",
    "\n",
    "    '''分割训练集和测试集'''\n",
    "    rarray=np.random.random(size=line_count)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in range(line_count-1):\n",
    "        if rarray[i]<0.8:\n",
    "            x_train.append(tfidf_matrix[i,:])\n",
    "            y_train.append(labels[i])\n",
    "        else:\n",
    "            x_test.append(tfidf_matrix[i,:])\n",
    "            y_test.append(labels[i])\n",
    "\n",
    "    # x_train,x_test,y_train,y_test = train_test_split(tfidf_matrix,labels,test_size=0.3,random_state=0)\n",
    "\n",
    "    '''LR模型分类训练'''\n",
    "    classifier=LogisticRegression()\n",
    "\n",
    "    classifier.fit(x_train, y_train)\n",
    "    #\n",
    "    # with open('LR_model.pkl', 'wb') as f:\n",
    "    #     pickle.dump(classifier, f)\n",
    "\n",
    "    print(classification_report(y_test,classifier.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
